# PRISM Default Configuration
#
# Override settings by copying this file and modifying values.
# Environment variables can also override these settings:
#   PRISM_LLM_ENDPOINT, PRISM_LLM_MODEL, PRISM_LLM_TIMEOUT, PRISM_LOG_LEVEL

llm:
  # Ollama API endpoint
  endpoint: "http://localhost:11434"

  # Model to use for agent inference
  # Options: mistral (fast/dev), llama3 (quality), llama3:70b (best quality)
  model: "mistral"

  # Reasoning effort affects response quality vs speed
  # Options: low (fast), medium (balanced), high (thorough)
  reasoning_effort: "medium"

  # Request timeout in seconds
  timeout: 30

  # Sampling temperature (0.0 = deterministic, 2.0 = creative)
  temperature: 0.7

agent:
  # Default personality for agents without explicit personality
  default_personality: "curious and engaged social media user"

logging:
  # Logging level: DEBUG, INFO, WARNING, ERROR
  level: "INFO"
