llm:
  provider: ollama
  host: "http://localhost:11434"
  model_id: mistral
  temperature: 0.7
  max_tokens: 512
  seed: null
