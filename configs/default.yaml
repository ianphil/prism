llm:
  provider: ollama                # LLM backend provider (currently only "ollama" supported)
  host: "http://localhost:11434"  # Ollama server URL
  model_id: mistral               # Model to use (e.g., mistral, llama2, qwen2.5)
  temperature: 0.7                # Sampling randomness (0.0=deterministic, 2.0=very random)
  max_tokens: 512                 # Maximum tokens in LLM response
  seed: null                      # Random seed for reproducibility (null=non-deterministic)
